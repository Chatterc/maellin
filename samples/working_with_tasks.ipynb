{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "import pandas as pd\n",
    "from tasks import BaseTask as Task\n",
    "from pandas.testing import assert_frame_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== PARAMETERS ====================== #\n",
    "# First lets reference some data files for us to work with\n",
    "dimA = 'data/dim_a.csv'\n",
    "dimB = 'data/dim_b.csv'\n",
    "dimC = 'data/dim_c.csv'\n",
    "factTbl = 'data/fact_tbl.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== FUNCTIONS ====================== #\n",
    "# Now lets create some python functions that we will use to process some data\n",
    "def select_target_cols(df: pd.DataFrame, target_schema: List) -> pd.DataFrame:\n",
    "    df = df[target_schema]\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_composite_key(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['CompositeKey'] = df.CNo.map(str) + \"-\" \\\n",
    "        + df.BNo.map(str) + \"-\" \\\n",
    "        + df.ANo.map(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_table(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def join_dataframes(A: pd.DataFrame, B: pd.DataFrame, on: Union[str, List], how: str) -> pd.DataFrame:\n",
    "    df = A.merge(B, on=on, how=how)\n",
    "    return df\n",
    "\n",
    "\n",
    "def sink_output(df: pd.DataFrame, path: str) -> None:\n",
    "    df.to_csv(path)\n",
    "    return\n",
    "\n",
    "\n",
    "def drop_duplicates(df: pd.DataFrame, columns=List[str]) -> pd.DataFrame:\n",
    "    df = df.drop_duplicates(subset=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== TASKS ====================== #\n",
    "\n",
    "# Now lets create a bunch of tasks by wrapping our functions\n",
    "read = Task(read_table)\n",
    "select = Task(select_target_cols)\n",
    "join = Task(join_dataframes)\n",
    "joinCust = Task(join_dataframes)\n",
    "joinBG = Task(join_dataframes)\n",
    "composite = Task(create_composite_key)\n",
    "sink = Task(sink_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================== EXECUTION ====================== #\n",
    "\n",
    "# Now that we have some tasks we can use some of the methods we defined\n",
    "# Lets see our validate method in action by using it\n",
    "select.validate(read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "CompatibilityException",
     "evalue": "Validation Failed. Output of select_target_cols is incompatible with inputs from read_table",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCompatibilityException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Lets see what happens when we try to validate to tasks that are not compatible\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m read\u001b[39m.\u001b[39;49mvalidate(select)\n",
      "File \u001b[1;32mc:\\Users\\Chatt\\OneDrive\\Desktop\\maellin\\maellin\\tasks.py:94\u001b[0m, in \u001b[0;36mBaseTask.validate\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mif\u001b[39;00m _val \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     error \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidation Failed. Output of \u001b[39m\u001b[39m{\u001b[39;00mother\u001b[39m.\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[0;32m     93\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis incompatible with inputs from \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 94\u001b[0m     \u001b[39mraise\u001b[39;00m CompatibilityException(error)\n\u001b[0;32m     96\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mCompatibilityException\u001b[0m: Validation Failed. Output of select_target_cols is incompatible with inputs from read_table"
     ]
    }
   ],
   "source": [
    "# Lets see what happens when we try to validate to tasks that are not compatible\n",
    "read.validate(select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AId_x     BId  AId_y\n",
      "0    300  784955    300\n",
      "1    300  784955    406\n",
      "2    300  784955    412\n",
      "3    300  784955    784\n",
      "4    300  784955    787\n",
      "5    300  784955    842\n",
      "6    300  784955   1030\n",
      "7    300  784955   1034\n",
      "8    300  784955   1064\n",
      "9    300  784955   1156\n"
     ]
    }
   ],
   "source": [
    "# Lets run some tasks we created in a linear (sequential pipeline)\n",
    "df = read.run(path='data\\dim_a.csv')\n",
    "df = select.run(df, ['AId', 'BId'])\n",
    "df = join.run(df, df, on='BId', how='left')\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Postgres Client with Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from psycopg import Connection\n",
    "from clients.postgres import PostgresClient\n",
    "from tasks import BaseTask as Task\n",
    "\n",
    "from typing import List, Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets write some functions to use for processing data from our DVD-Rental Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets pack all this into a function that we can encapsulate within a set of task\n",
    "def setup_client(config_file, section) -> Connection:\n",
    "    client = PostgresClient()\n",
    "    cursor = client.connect_from_config(path=config_file, section=section)\n",
    "    return cursor\n",
    "\n",
    "def set_search_path(cursor, catalog, schema) -> Connection:\n",
    "    cursor.execute(f\"SET search_path TO {catalog}, {schema};\")\n",
    "    return cursor\n",
    "    \n",
    "def create_new_schema(cursor, schema) -> Connection:\n",
    "    cursor.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema};\")\n",
    "    return cursor\n",
    "    \n",
    "def read_table(cursor, table) -> List[Tuple]:\n",
    "    tbl = cursor.execute(f\"SELECT * FROM {table};\").fetchall()\n",
    "    return tbl\n",
    "    \n",
    "def to_pandas_df(data) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Lets Encapsulate Those functions in as Pipeline Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can put our functions in tasks for our pipeline\n",
    "tsk_0 = Task(setup_client)\n",
    "tsk_1 = Task(set_search_path)\n",
    "tsk_2 = Task(create_new_schema)\n",
    "tsk_3 = Task(read_table)\n",
    "tsk_4 = Task(to_pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets execute this as a linear pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Penelope</td>\n",
       "      <td>Guiness</td>\n",
       "      <td>2013-05-26 14:47:57.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Nick</td>\n",
       "      <td>Wahlberg</td>\n",
       "      <td>2013-05-26 14:47:57.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Ed</td>\n",
       "      <td>Chase</td>\n",
       "      <td>2013-05-26 14:47:57.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Davis</td>\n",
       "      <td>2013-05-26 14:47:57.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Johnny</td>\n",
       "      <td>Lollobrigida</td>\n",
       "      <td>2013-05-26 14:47:57.620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1             2                       3\n",
       "0  1  Penelope       Guiness 2013-05-26 14:47:57.620\n",
       "1  2      Nick      Wahlberg 2013-05-26 14:47:57.620\n",
       "2  3        Ed         Chase 2013-05-26 14:47:57.620\n",
       "3  4  Jennifer         Davis 2013-05-26 14:47:57.620\n",
       "4  5    Johnny  Lollobrigida 2013-05-26 14:47:57.620"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor = tsk_0.run(\"..\\..\\maellin\\.config\\.postgres\", 'postgresql')\n",
    "cursor = tsk_1.run(cursor=cursor, catalog='dvdrental', schema='public')\n",
    "data = tsk_3.run(cursor=cursor, table='actor')\n",
    "df = tsk_4.run(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But What about Task 2 that we did not use?\n",
    "\n",
    "Task 2 is for creating a new schema, possibly a target for our processed data to use as a sink.\n",
    "But putting task 2 in our linear pipeline feels awkward, it does not fit in well with the other steps.\n",
    "This is where pipeline branching (splitting the pipeline into separate directions becomes useful)\n",
    "\n",
    "## To use branching strategies in data processing we need a DAG!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93f728960881c0e95a58ff4f43d965588a90ef8f40f993ba145d129978fecb3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
